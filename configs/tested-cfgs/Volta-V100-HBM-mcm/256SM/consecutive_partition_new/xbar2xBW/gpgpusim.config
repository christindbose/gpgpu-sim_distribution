# functional simulator specification
-gpgpu_ptx_instruction_classification 0
-gpgpu_ptx_sim_mode 0
-gpgpu_ptx_force_max_capability 60 

# SASS execution (only supported with CUDA >= 4.0)
-gpgpu_ptx_convert_to_ptxplus 0
-gpgpu_ptx_save_converted_ptxplus 0

# high level architecture configuration
-gpgpu_n_clusters 128
-gpgpu_n_cores_per_cluster 2
-gpgpu_n_mem 128
-gpgpu_n_sub_partition_per_mchannel 1 

# Pscal clock domains
#-gpgpu_clock_domains <Core Clock>:<Interconnect Clock>:<L2 Clock>:<DRAM Clock>
# Pascal NVIDIA GP100  clock domains are adopted from 
# https://en.wikipedia.org/wiki/Nvidia_Tesla
-gpgpu_clock_domains 1480.0:2960.0:1480.0:715.0
-scale_down_on_chip_traffic 2

# shader core pipeline config
-gpgpu_shader_registers 65536

# This implies a maximum of 64 warps/SM
-gpgpu_shader_core_pipeline 2048:32 
-gpgpu_shader_cta 32
-gpgpu_simd_model 1 

# Pipeline widths and number of FUs
# ID_OC_SP,ID_OC_DP,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_DP,OC_EX_SFU,OC_EX_MEM,EX_WB
## Pascal GP100 has 2 SP SIMD units, 2 SFU units, 2 DP units per core
## we need to scale the number of pipeline registers to be equal to the number of SP units
-gpgpu_pipeline_widths 4,4,4,1,4,4,4,1,12
-gpgpu_num_sp_units 4
-gpgpu_num_sfu_units 4
-gpgpu_num_dp_units 4

# Instruction latencies and initiation intervals
# "ADD,MAX,MUL,MAD,DIV"
# All Div operations are executed on SFU unit
# Throughput (initiation latency) are adopted from CUDA SDK document V8, section 5.4.1, Table 2
-ptx_opcode_latency_int 4,13,4,5,145
-ptx_opcode_initiation_int 1,1,1,1,4
-ptx_opcode_latency_fp 4,13,4,5,39
-ptx_opcode_initiation_fp 1,2,1,1,4
-ptx_opcode_latency_dp 8,19,8,8,330
-ptx_opcode_initiation_dp 2,2,2,2,130
-ptx_opcode_latency_sfu 8
-ptx_opcode_initiation_sfu 4


# <nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>:<set_index_fn>,<mshr>:<N>:<merge>,<mq>:**<fifo_entry>
# ** Optional parameter - Required when mshr_type==Texture Fifo
# Note: Hashing set index function (H) only applies to a set size of 32 or 64. 
# Pascal GP100 has 64KB Shared memory
-gpgpu_cache:dl1  S:64:128:8,L:L:f:N:H,A:256:8,16:0,32
-gpgpu_cache:dl1PrefL1  S:64:128:16,L:L:f:N:H,A:256:8,16:0,32
-gpgpu_cache:dl1PrefShared  S:32:128:6,L:L:f:N:H,A:256:8,16:0,32
-gpgpu_shmem_size 65536
-gpgpu_shmem_size_PrefL1 1
-gpgpu_shmem_size_PrefShared 98304
-gmem_skip_L1D 1
-icnt_flit_size 40
-gpgpu_n_cluster_ejection_buffer_size 32

# 32 sets, each 128 bytes 16-way for each memory sub partition (128 KB per memory sub partition). This gives  4MB L2 cache
-gpgpu_cache:dl2 S:64:128:16,L:B:m:L:L,A:256:4,32:0,32
-gpgpu_cache:dl2_texture_only 0 
-gpgpu_dram_partition_queues 64:64:64:64
-perf_sim_memcpy 0
-memory_partition_indexing 0

# 4 KB Inst.
-gpgpu_cache:il1 N:8:128:4,L:R:f:N:L,S:2:48,4
# 48 KB Tex 
-gpgpu_tex_cache:l1 N:16:128:24,L:R:m:N:L,F:128:4,128:2
# 12 KB Const
-gpgpu_const_cache:l1 N:128:64:2,L:R:f:N:L,S:2:64,4

# enable operand collector 
-gpgpu_operand_collector_num_units_sp 12
-gpgpu_operand_collector_num_units_sfu 6
-gpgpu_operand_collector_num_units_mem 8
-gpgpu_operand_collector_num_units_dp 6
-gpgpu_operand_collector_num_in_ports_sp 4
-gpgpu_operand_collector_num_out_ports_sp 4
-gpgpu_operand_collector_num_in_ports_sfu 1
-gpgpu_operand_collector_num_out_ports_sfu 1
-gpgpu_operand_collector_num_in_ports_mem 1
-gpgpu_operand_collector_num_out_ports_mem 1
-gpgpu_operand_collector_num_in_ports_dp 1
-gpgpu_operand_collector_num_out_ports_dp 1
# gpgpu_num_reg_banks should be increased to 32
-gpgpu_num_reg_banks 32

# shared memory bankconflict detection 
-gpgpu_shmem_num_banks 32
-gpgpu_shmem_limited_broadcast 0
-gpgpu_shmem_warp_parts 1
-gpgpu_coalesce_arch 60

## In Pascal, a warp scheduler can issue 2 insts per cycle using 2 diff execution units
-gpgpu_max_insn_issue_per_warp 2
-gpgpu_dual_issue_diff_exec_units 1

# interconnection
-network_mode 1 
-inter_config_file config_fermi_islip.icnt

# memory partition latency config 
-rop_latency 120
# DRAM latency should be lower compared to other configs, due to high-speed interposer connection
-dram_latency 100

# dram model config
-gpgpu_dram_scheduler 1
# The DRAM return queue and the scheduler queue together should provide buffer
# to sustain the memory level parallelism to tolerate DRAM latency 
# To allow 100% DRAM utility, there should at least be enough buffer to sustain
# the minimum DRAM latency (100 core cycles).  I.e. 
#   Total buffer space required = 100 x 924MHz / 700MHz = 132
-gpgpu_frfcfs_dram_sched_queue_size 64
-gpgpu_dram_return_queue_size 192

# for HBM, 32 channles, each (128 bits) 16 bytes width
-gpgpu_n_mem_per_ctrlr 1
-gpgpu_dram_buswidth 16
-gpgpu_dram_burst_length 2
-dram_data_command_freq_ratio 2  # HBM is DDR
-gpgpu_mem_address_mask 1
-gpgpu_mem_addr_mapping dramid@8;00000000.00000000.00000000.00000000.0000RRRR.RRRRRRRR.RBBBBCCC.CCCSSSSS

# HBM timing are adopted from hynix JESD235 standered and nVidia HPCA 2017 paper (http://www.cs.utah.edu/~nil/pubs/hpca17.pdf)
# Timing for 1 GHZ
# tRRDl and tWTR are missing, need to be added
-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=4:RCD=14:RAS=33:RP=14:RC=47:
                        CL=14:WL=2:CDLR=3:WR=12:nbkgrp=4:CCDL=2:RTPL=4"

# Timing for 715 MHZ, Tesla Pascal P100 HBM runs at 715 MHZ
#-gpgpu_dram_timing_opt "nbk=16:CCD=1:RRD=3:RCD=10:RAS=24:RP=10:RC=34:
#                        CL=10:WL=2:CDLR=3:WR=9:nbkgrp=4:CCDL=2:RTPL=3"

# HBM has dual bus interface, in which it can issue two col and row commands at a time
-dual_bus_interface 1
# select lower bits for bnkgrp to increase bnkgrp parallelism
-dram_bnk_indexing_policy 0
-dram_bnkgrp_indexing_policy 1

#-Seperate_Write_Queue_Enable 1
#-Write_Queue_Size 64:56:32

# Pascal has two schedulers per core
-gpgpu_num_sched_per_core 4
# Two Level Scheduler with active and pending pools
#-gpgpu_scheduler two_level_active:6:0:1
# Loose round robbin scheduler
#-gpgpu_scheduler lrr
# Greedy then oldest scheduler
-gpgpu_scheduler gto

# stat collection
-gpgpu_memlatency_stat 14 
-gpgpu_runtime_stat 500
-enable_ptx_file_line_stats 1
-visualizer_enabled 0

# power model configs, disable it untill we create a real energy model for Pascal 100
-power_simulation_enabled 0
-gpuwattch_xml_file gpuwattch_gtx480.xml

# tracing functionality
#-trace_enabled 1
#-trace_components WARP_SCHEDULER,SCOREBOARD
#-trace_sampling_core 0

# Multichip config
#if you change the parition mapping, ensure you change the any_net file as well
#if FT policy is used, then it is impo to use parition_mapping = 0 (i.e. consecutive)
-multi_chip_mode 1
-n_gpu_chips 4
-mcm_partition_mapping 0
-mcm_vm_ft_policy 0
-mcm_vm_pagesize 8192
-mcm_vm_ft_latency 0

#0: naive, 1:caorse (specify the grain by mcm_cta_sched_grain), 2:row, 3:col
#for, col sched you need to decrease the page size by a factor (e.g. 4)
-mcm_coarse_grain_cta_sched 0
-mcm_cta_sched_grain 1
-mcm_cta_data_type_size 4
-restrictive_RR_assignement 1

-cache_remote_data 1
-m_n_external 128
-gpgpu_cache:dr2 S:64:128:8,L:E:m:N:L,A:256:4,32:0,32
-write_bypass_remote_cache 1
-cache_remote_only_once 0
-remote_cache_latency 50
-offchiplet_latency 200
-gpgpu_flush_l1_cache 1

-gpgpu_cache:dl2 S:64:128:8,L:B:m:L:L,A:256:4,32:0,32
